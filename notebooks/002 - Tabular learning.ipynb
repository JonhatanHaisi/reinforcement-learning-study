{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is tabular learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular learning in reinforcement learning refers to methods that store and update value estimates for each state or state-action pair in a table. Algorithms like Q-learning and SARSA use this approach to iteratively refine value functions based on experience. It is effective for small state spaces but becomes impractical for large or continuous environments due to memory and computation constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from utils.wrappers import RecordGif\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-Iteraction learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating the agent responsible by learn how to navigate in the frozen lake to get the reward at the end of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP SOME CONSTANTS FOR THE AGENT\n",
    "GAMMA = 0.9\n",
    "\n",
    "# DEFINING SOME TYPE ALIASES\n",
    "State = int\n",
    "Action = int\n",
    "RewardKey = tt.Tuple[State, Action, State]\n",
    "TransictionKey = tt.Tuple[State, Action]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env: gym.Env):\n",
    "        self.env = env\n",
    "        self.state, _ = env.reset()\n",
    "\n",
    "        self.rewards: tt.Dict[RewardKey, float] = defaultdict(float)\n",
    "        self.transictions: tt.Dict[TransictionKey, Counter] = defaultdict(Counter)\n",
    "        self.values: tt.Dict[State, float] = defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, n: int):\n",
    "        for _ in range(n):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, done, trunc, _ = self.env.step(action)\n",
    "            self.rewards[self.state, action, new_state] = float(reward)\n",
    "            self.transictions[self.state, action][new_state] += 1\n",
    "\n",
    "            if done or trunc:\n",
    "                self.state, _ = self.env.reset()\n",
    "            else:\n",
    "                self.state = new_state\n",
    "\n",
    "\n",
    "    def calc_action_value(self, state: State, action: Action) -> float:\n",
    "        target_state_counts = self.transictions[state, action]\n",
    "        total_visits = sum(target_state_counts.values())\n",
    "\n",
    "        if total_visits == 0:\n",
    "            return 0.0\n",
    "\n",
    "        action_value = sum(\n",
    "            target_state_counts[target_state] * (self.rewards[state, action, target_state] + GAMMA * self.values[target_state])\n",
    "            for target_state in target_state_counts\n",
    "        ) / total_visits\n",
    "\n",
    "        return action_value\n",
    "\n",
    "\n",
    "    def select_action(self, state: State) -> Action:\n",
    "        action_values = [\n",
    "            self.calc_action_value(state, action)\n",
    "            for action in range(self.env.action_space.n)\n",
    "        ]\n",
    "        return np.argmax(action_values)\n",
    "    \n",
    "    \n",
    "    def update_values(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action)\n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "            self.values[state] = max(state_values)\n",
    "\n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, done, trunc, _ = env.step(action)\n",
    "            \n",
    "            self.rewards[state, action, new_state] = float(reward)\n",
    "            self.transictions[state, action][new_state] += 1\n",
    "\n",
    "            total_reward += reward\n",
    "            if done or trunc:\n",
    "                return total_reward\n",
    "            state = new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the `__init__` function, we need to initialize some dicts we will use to train our agent:\n",
    "* `rewards`: This dict is responsible by store the reward for every agent step. We need to save the rewards so we can use it to find the path who will provide the higgest prize to the agent.\n",
    "* `transictions`: We are also storing the state transictions and count the occurences as the agent needs this information to calculate the best path to the best reward.\n",
    "* `values`: These are the state values and this is the result of the training and the data the agent will use to decide the acions it should take.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `play_r_random_steps` is used to randomly play the environment and collect data for every step. This is the function we need to run to collect all the data we need to train the agent.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `calc_action_value` function is one of the most important functions for Q-Value learning. The value of the action is the value the agent will use to select the action it has to perform in every situattion. The higgest the value action should the the action selected by the agent to do in any state.\n",
    "\n",
    "$$\n",
    "V(s, a) = \\frac{1}{N(s, a)} \\sum_{s'} T(s, a, s') \\left[ R(s, a, s') + \\gamma V(s') \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $( V(s, a) )$ is the action value for action $( a )$ in state $( s )$.\n",
    "- $( N(s, a) )$ is the total number of times action $( a )$ has been taken in state $( s )$.\n",
    "- $( T(s, a, s') )$ is the count of transitions from state $( s )$ to state $( s' )$ after taking action $( a )$.\n",
    "- $( R(s, a, s') )$ is the reward received after transitioning from state $( s )$ to state $( s' )$ with action $( a )$.\n",
    "- $( \\gamma )$ is the discount factor.\n",
    "- $( V(s') )$ is the value of the next state $( s' )$.\n",
    "\n",
    "The function calculates the expected value of the action by averaging the rewards and discounted future values over all possible next states.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select_action` function is responsible by iterate over all possible actions, calculate the action value and select the action with higgest value.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `update_values` function is other of the most important functions, as it is responsible by calculate the value of the state, and update the `values` dictionary.\n",
    "\n",
    "$$\n",
    "V(s) = \\max_{a} Q(s, a)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $( V(s) )$ is the state value $( s )$.\n",
    "- $( \\max_{a} Q(s, a) )$ is the max action value $( a )$ in the state $( s )$.\n",
    "\n",
    "The function iterates over all states and calculates the value of each state as the maximum value of all possible actions in that state.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `play_episode` will play an episode of the game, using the trained action and state values calulated preciously by the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated: 0.000 -> 0.200\n",
      "Best reward updated: 0.200 -> 0.500\n",
      "Best reward updated: 0.500 -> 0.850\n",
      "Solved in 11 iteractions!\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'FrozenLake-v1'\n",
    "\n",
    "episode_trigger = lambda x: x % 250 == 0\n",
    "env = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "env = RecordGif(env, './gifs/v-iteraction/frozen-lake', name_prefix='traning', gif_length=500, episode_trigger=episode_trigger)\n",
    "\n",
    "agent = Agent(env)\n",
    "writer = SummaryWriter(logdir='runs/v-iteraction/frozen-lake', comment=f'-frozen-lake-q-value')\n",
    "\n",
    "iteraction_number = 0\n",
    "best_reward = 0.0\n",
    "\n",
    "while True:\n",
    "    iteraction_number += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.update_values()\n",
    "\n",
    "    reward = np.mean([agent.play_episode(env) for _ in range(20)])\n",
    "    writer.add_scalar('reward', reward, iteraction_number)\n",
    "\n",
    "    if reward > best_reward:\n",
    "        print(f'Best reward updated: {best_reward:.3f} -> {reward:.3f}')\n",
    "        best_reward = reward\n",
    "\n",
    "    if reward > 0.80:\n",
    "        print(f'Solved in {iteraction_number} iteractions!')\n",
    "        break\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! V-Iteraction could learn how to solve the frozen lake really fast!\n",
    "\n",
    "This is how it started!<br>\n",
    "<img src=\"./gifs/v-iteraction/frozen-lake/traning-episode-0.gif\" style=\"width: 250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last recorded run!<br>\n",
    "<img src=\"./gifs/v-iteraction/frozen-lake/traning-episode-250.gif\" style=\"width: 250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try again, but using only the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "env = RecordGif(env, './gifs/v-iteraction/frozen-lake', name_prefix='model', gif_length=500)\n",
    "\n",
    "agent.play_episode(env)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./gifs//q-value/frozen-lake/model-episode-0.gif\">\n",
    "\n",
    "Even with the complexity of the environment who drift the step some times, it could perform really well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Iteraction learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP SOME CONSTANTS FOR THE AGENT\n",
    "GAMMA = 0.9\n",
    "\n",
    "# DEFINING SOME TYPE ALIASES\n",
    "State = int\n",
    "Action = int\n",
    "RewardKey = tt.Tuple[State, Action, State]\n",
    "TransictionKey = tt.Tuple[State, Action]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env: gym.Env):\n",
    "        self.env = env\n",
    "        self.state, _ = env.reset()\n",
    "\n",
    "        self.rewards: tt.Dict[RewardKey, float] = defaultdict(float)\n",
    "        self.transictions: tt.Dict[TransictionKey, Counter] = defaultdict(Counter)\n",
    "        self.values: tt.Dict[TransictionKey, float] = defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, n: int):\n",
    "        for _ in range(n):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, done, trunc, _ = self.env.step(action)\n",
    "            self.rewards[self.state, action, new_state] = float(reward)\n",
    "            self.transictions[self.state, action][new_state] += 1\n",
    "\n",
    "            if done or trunc:\n",
    "                self.state, _ = self.env.reset()\n",
    "            else:\n",
    "                self.state = new_state\n",
    "\n",
    "\n",
    "    def select_action(self, state: State) -> Action:\n",
    "        action_values = [\n",
    "            self.values[state, action]\n",
    "            for action in range(self.env.action_space.n)\n",
    "        ]\n",
    "        return np.argmax(action_values)\n",
    "    \n",
    "    \n",
    "    def update_values(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                target_state_counts = self.transictions[state, action]\n",
    "                total_visits = sum(target_state_counts.values())\n",
    "\n",
    "                if total_visits == 0:\n",
    "                    continue\n",
    "\n",
    "                action_value = 0.0\n",
    "                for target_state, count in target_state_counts.items():\n",
    "                    best_target_action = self.select_action(target_state)\n",
    "                    value = self.rewards[state, action, target_state] + GAMMA * self.values[target_state, best_target_action]\n",
    "                    action_value += (count/total_visits) * value\n",
    "\n",
    "                self.values[state, action] = action_value\n",
    "\n",
    "\n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, done, trunc, _ = env.step(action)\n",
    "            \n",
    "            self.rewards[state, action, new_state] = float(reward)\n",
    "            self.transictions[state, action][new_state] += 1\n",
    "\n",
    "            total_reward += reward\n",
    "            if done or trunc:\n",
    "                return total_reward\n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated: 0.000 -> 0.100\n",
      "Best reward updated: 0.100 -> 0.150\n",
      "Best reward updated: 0.150 -> 0.250\n",
      "Best reward updated: 0.250 -> 0.400\n",
      "Best reward updated: 0.400 -> 0.500\n",
      "Best reward updated: 0.500 -> 0.650\n",
      "Best reward updated: 0.650 -> 0.800\n",
      "Best reward updated: 0.800 -> 0.850\n",
      "Solved in 81 iteractions!\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'FrozenLake-v1'\n",
    "\n",
    "episode_trigger = lambda x: x % 250 == 0\n",
    "env = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "env = RecordGif(env, './gifs/q-iteraction/frozen-lake', name_prefix='traning', gif_length=500, episode_trigger=episode_trigger)\n",
    "\n",
    "agent = Agent(env)\n",
    "writer = SummaryWriter(logdir='runs/q-iteraction/frozen-lake', comment=f'-frozen-lake-q-value')\n",
    "\n",
    "iteraction_number = 0\n",
    "best_reward = 0.0\n",
    "\n",
    "while True:\n",
    "    iteraction_number += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.update_values()\n",
    "\n",
    "    reward = np.mean([agent.play_episode(env) for _ in range(20)])\n",
    "    writer.add_scalar('reward', reward, iteraction_number)\n",
    "\n",
    "    if reward > best_reward:\n",
    "        print(f'Best reward updated: {best_reward:.3f} -> {reward:.3f}')\n",
    "        best_reward = reward\n",
    "\n",
    "    if reward > 0.80:\n",
    "        print(f'Solved in {iteraction_number} iteractions!')\n",
    "        break\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Iteraction could learn how to solve the frozen lake fast as well!\n",
    "\n",
    "This is how it started!<br>\n",
    "<img src=\"./gifs/q-iteraction/frozen-lake/traning-episode-0.gif\" style=\"width: 250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last recorded run!<br>\n",
    "<img src=\"./gifs/q-iteraction/frozen-lake/traning-episode-2500.gif\" style=\"width: 250px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

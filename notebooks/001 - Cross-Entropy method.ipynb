{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Cross-Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy is a way to measure how different two probability distributions are. Imagine you have a model that makes predictions, and you want to see how close these predictions are to the actual outcomes. Cross-Entropy gives a number that tells you how good your model is. A lower cross-entropy means your model's predictions are better.\n",
    "\n",
    "**Simple Example:**\n",
    "\n",
    "* Think of predicting the weather. If you say there's a 70% chance of rain and it actually rains, your prediction is pretty good. Cross-Entropy helps measure how accurate such predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does RL Use Cross-Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Reinforcement Learning (RL), an agent learns to make decisions by performing actions and receiving rewards. When RL uses Cross-Entropy, it helps the agent improve its actions based on past experiences.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "* The agent tries different actions in an environment.\n",
    "* It records which actions lead to good rewards.\n",
    "* Using Cross-Entropy, it focuses more on the actions that gave better rewards.\n",
    "* Over time, the agent gets better at choosing actions that maximize rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Step-by-Step Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s a simple step-by-step guide on how RL with Cross-Entropy works:\n",
    "\n",
    "1. **Initialize:** Start with a random policy (a set of rules the agent follows to decide actions).\n",
    "2. **Generate Episodes:** Let the agent perform actions in the environment to create episodes (sequences of states, actions, and rewards).\n",
    "3. **Evaluate:** Calculate the total rewards for each episode.\n",
    "Select Top Performers: Choose the best-performing episodes based on their rewards.\n",
    "4. **Update Policy:** Use Cross-Entropy to update the policy, making it more likely to choose actions that led to high rewards.\n",
    "5. **Repeat:** Go back to step 2 and repeat the process until the agent performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantages:\n",
    "    * Simplicity: The Cross-Entropy method is straightforward and easy to implement.\n",
    "    * Efficiency: It can find good solutions with fewer trials compared to other methods.\n",
    "\n",
    "\n",
    "* Applications:\n",
    "    * Robotics (teaching robots to perform tasks)\n",
    "    * Game Playing (like training agents to play video games)\n",
    "    * Optimization Problems (finding the best solutions in complex scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I would like to check the graphics card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 13 12:02:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   60C    P8              4W /   60W |     505MiB /   6144MiB |      8%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      6700    C+G   ...pplications\\Tryd6\\jre\\bin\\javaw.exe      N/A      |\n",
      "|    0   N/A  N/A     20532    C+G   ...ROGRA~2\\Citrix\\ICACLI~1\\wfica32.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Sep_12_02:55:00_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.77\n",
      "Build cuda_12.6.r12.6/compiler.34841621_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n",
      "Is CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print('Is CUDA available: ' + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import namedtuple\n",
    "from tensorboardX import SummaryWriter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CartPole is one of the most basic environments available on gymnasium. This environment is well suit suitable to be used on this first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from utils.wrappers import RecordGif\n",
    "\n",
    "# RUN A RANDOM AGENT FOR DEMO PURPOSES\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env = RecordGif(env, f'./gifs/cross-entroph/cartpole', gif_length=200, name_prefix='random-agent')\n",
    "\n",
    "env.reset(seed=42)\n",
    "for _ in range(200):\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"gifs/cross-entroph/cartpole/random-agent-episode-0.gif\" style=\"width: 350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a video of the cart pole environment being controlled by random actions.<br>\n",
    "This environment consists in a pole being balanced by a cart who can move to left or right to keep the pole in a vertical position.<br>\n",
    "The apisode ends when the pole cross a given angle, or the cart cross the environment boundaries. For every step the agent can keep the pole balanced, it will receive a positive reward.\n",
    "\n",
    "\n",
    "The environment could look simple, but it is a good candidate to implement a reinforcement learning, cross-entropy, model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the function to yield the batches to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to create the batches of data to train the model. The batches are set by running the Neural Network throught the environment steps, acumulate the rewards and group them in collections to use during the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These named tuples are to store the experience of the agent\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "# this is the function to create the batches\n",
    "def training_batch(env: gym.Env, net: nn.Module, batch_size: int, device: str):\n",
    "    '''\n",
    "    This function will generate a batch of episodes from the environment.\n",
    "    The batch will be of size batch_size.\n",
    "    The neural network nn will be used to generate the actions.\n",
    "    The device will be used to store the tensors.\n",
    "\n",
    "    The function will yield a list of episodes of size batch_size.\n",
    "    '''\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    while True:\n",
    "        batchs = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            obs,_ = env.reset()\n",
    "\n",
    "            total_reward = 0.0\n",
    "            steps = []\n",
    "\n",
    "            while True:\n",
    "                obs_v = torch.FloatTensor([obs]).to(device)\n",
    "\n",
    "                act_probs_v = softmax(net(obs_v))\n",
    "                act_probs = act_probs_v.data.cpu().numpy()[0]\n",
    "                action = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "                next_obs, reward, done, truncated, _ = env.step(action)\n",
    "                steps.append(EpisodeStep(observation=obs, action=action))\n",
    "                \n",
    "                obs = next_obs\n",
    "                total_reward += reward\n",
    "\n",
    "                if done or truncated:\n",
    "                    batchs.append(Episode(reward=total_reward, steps=steps))\n",
    "                    break\n",
    "        \n",
    "        yield batchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Episode**: This tuple was defined to store the information of every episode. So for every run where our neural network is interacting with the environment we are acumulating the data in a instance of this tuple. The 'reward' field receives the total reward obtained during the episode, and the 'steps' field is to store a list of *EpisodeStep*.\n",
    "\n",
    "**EpisodeStep**: This tuple was defined to store the information of a single step from the agent in the environment. So for every step we needs to create a new instance of this tuple to include in the *Episode*. The 'action' field is to store the action from the agent in the step, and the 'observation' field is to store the observation the agend did use to choose the action.\n",
    "\n",
    "> It is important to notice here, the opservation we are includding in the EpisodeStep is the 'obs' the agent did use to choose an action and not the 'next_obs' we receive after the step.\n",
    "\n",
    "**training_batch**: This function has the singlÃ§e objective of run some episodes using the Neural Network agent to choose an action for every step from the episodes. Than it should acumulate the data from the episodes and return it as an yield value. We can use this function to get the batches we need during the training of the agent.\n",
    "\n",
    "> You can see something interesting in the training_batch function. When we are choosing an action to step into the evrironment, we are not using the direct result from the Neural Network, we are using the probabilities generated by the Softmax activation function, and we are using these probabilities to choose a random action based on them. So while the agente is getting more confident to perform an action for a given scenario, this action will have more chances to be the chosen while we keep space for exploration.\n",
    "\n",
    "> Another interesting point is we are using the Softmax outside of the newral network! We are doing it, because we not expecting to use it on our final agent, but it is supporting us to get the probabilities for every action, for every step, during the batch creation.\n",
    "\n",
    "\n",
    "Ok, now we have a way to get batches to use during the training. But we still have to filter the top episodes to discard the ones with a bad performance. This is exactly what we are going to do next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the episodes to use for model training (choose the Elite episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our agent will learn on good experences and ignore the bad ones, we have to... we... discard the bad experiences! We are looking for the Elite ones!\n",
    "\n",
    "A technique to filter off the bad experiences is to define a minimun reward for an episode to include it on our training data. But should not be just an abritary value, and it has to adapt for every batch, since the average performance should increace over the time.\n",
    "\n",
    "We can, instead of an abitrary value, organize the rewards in percentiles, and choose only the episodes whos reward is equals or higger than the value related to a given percentile we choose. So with this strategy, the value we use to filter episodes will adapt for every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch: list[Episode], device: str, percentile:int = 70):\n",
    "    '''\n",
    "    This function will filter the batch of episodes.\n",
    "    It will only keep the episodes that have a reward greater than the percentile.\n",
    "    It will return the observations and the actions of the episodes.\n",
    "    '''\n",
    "    rewards = [ e.reward for e in batch ]\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "\n",
    "    for episode in batch:\n",
    "        if episode.reward <= reward_bound:\n",
    "            continue\n",
    "\n",
    "        train_obs.extend([step.observation for step in episode.steps])\n",
    "        train_act.extend([step.action for step in episode.steps])\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs).to(device)\n",
    "    train_act_v = torch.LongTensor(train_act).to(device)\n",
    "\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our function to filter batches is going to find the reward value related to the percentile (70 defined as default), use this percentile to filter episodes, acumulate the opservations and actions and return the observation, action, the reward bound (the reward related to the given percentile) and the reward mean.\n",
    "\n",
    "The reard bound and mean are just for metrics and they don't have any direct impact in the agent itself (well... we are going to use reward mean for early stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent (A Neural Network responsible by learn and run the episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define our agent! This is basically a neural retwork who is going to learn with experiences and play the episodes of Cart Pole game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Newral Network is pretty simple (we don't need anything to complex for Cart Pole game), we have 2 linear models, connected by a ReLU activation function.\n",
    "\n",
    "The model definition is arbritary and could be structured in different ways, but this one should work for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now all the pieces we need to start training our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.6793823838233948, reward_bound=24.0, reward_mean=24.0\n",
      "25: loss=0.5276786088943481, reward_bound=147.5, reward_mean=132.0625\n",
      "38: loss=0.49733400344848633, reward_bound=236.5, reward_mean=215.6875\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "# DEFINE THE HYPERPARAMETERS\n",
    "NN_HIDDEN_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# DEFINE THE ENVIRONMENT\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env = RecordGif(env, './gifs/cross-entroph/cartpole', name_prefix='training', gif_length=200, episode_trigger=lambda x: x % 500 == 0)\n",
    "\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# DEFINE THE NETWORK\n",
    "net = Net(observation_size, NN_HIDDEN_SIZE, action_size)\n",
    "\n",
    "# DEFINE THE OPTIMIZER\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# DEFINE THE LOSS FUNCTION\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# DEFINE THE DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net.to(device)\n",
    "\n",
    "# DEFINE THE WRITER\n",
    "writer = SummaryWriter(logdir='runs/cross-entroph/cart_pole', comment=f'-cartpole-pg')\n",
    "\n",
    "# TRAIN THE AGENT\n",
    "for iter_n, batch in enumerate(training_batch(env, net, BATCH_SIZE, device)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = loss_fn(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter_n % 25 == 0:\n",
    "        print(f'{iter_n}: loss={loss_v.item()}, reward_bound={reward_b}, reward_mean={reward_m}')\n",
    "\n",
    "    writer.add_scalar('loss', loss_v.item(), iter_n)\n",
    "    writer.add_scalar('reward_bound', reward_b, iter_n)\n",
    "    writer.add_scalar('reward_mean', reward_m, iter_n)\n",
    "\n",
    "    if reward_m > 199:\n",
    "        print(f'{iter_n}: loss={loss_v.item()}, reward_bound={reward_b}, reward_mean={reward_m}')\n",
    "        print('Solved!')\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took a while, but we have an Agent who can play the Cart Pole!\n",
    "\n",
    "How about compare some episodes?!\n",
    "\n",
    "**Episode 0:**<br>\n",
    "<image src=\"gifs/cross-entroph/cartpole/training-episode-0.gif\" style=\"width: 250px\">\n",
    "\n",
    "**Episode 500:**<br>\n",
    "<image src=\"gifs/cross-entroph/cartpole/training-episode-500.gif\" style=\"width: 250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking the metrics on tensorboad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorboard is a tool used to record and read metrics from our models. Let's see our metrics!\n",
    "\n",
    "<img src=\"./prints/cross-entroph/tensorboard-cartpole.png\" style=\"width: 1000px\">\n",
    "\n",
    "Using the tensorboard we can see the loss going down and the reward going up (reward is increacing almost linear hahahaha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "Path(r'models/cross-entroph').mkdir(exist_ok=True, parents=True)\n",
    "torch.save(net.state_dict(), 'models/cross-entroph/cartpole-pg.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a final video of the agent playing the cart pole game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env = RecordGif(env, './gifs/cross-entroph/cartpole', name_prefix='model', gif_length=500)\n",
    "\n",
    "net.load_state_dict(torch.load('models/cross-entroph/cartpole-pg.pth'))\n",
    "net.eval()\n",
    "\n",
    "obs,_ = env.reset(seed=42)\n",
    "while True:\n",
    "    obs_v = torch.FloatTensor([obs]).to(device)\n",
    "    action = torch.argmax(net(obs_v)).item()\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"gifs/cross-entroph/cartpole/model-episode-0.gif\" style=\"width: 350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is quite good!<br>\n",
    "It may not be able to run indefinidelly for some scenarios, since the agent never had to care about what happens after the 200 steps we did set as the reward mean to stop the training. Even with the limitation, we could produce a good agent!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import namedtuple\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frozen Lake is another of the most basic environments on GYM, but it has some differences from Cart Pole as we will discuss bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from utils.wrappers import RecordGif\n",
    "\n",
    "# RUN A RANDOM AGENT FOR DEMO PURPOSES\n",
    "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "env = RecordGif(env, f'./gifs/cross-entroph/frozen-lake', gif_length=15, name_prefix='random-agent', fps=3)\n",
    "\n",
    "np.random.seed(42)\n",
    "env.reset(seed=42)\n",
    "for _ in range(200):\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"gifs/cross-entroph/frozen-lake/random-agent-episode-0.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frozen lake environment, is a grid of 4X4 squares, where the agent should navigate to get the present. This is also very simple, but we have a big difference from the last environment. \n",
    "\n",
    "The Cart Pole the agent will receive a positive reward for every step the it can keep the pole balanced. But on the Frozen Lake environment, the agent will get the positive reward only when the gets the present. So it does not mater if the agent keeps walking arround and how much time it spend to get the present, the final reward will be the same.\n",
    "\n",
    "If you remember from cart pole, we have to filter the *Elite* episodes, and use these episodes to train the agent, but since the agent will receive the same reward independently of how many steps it stend to get the present. We need a way to reward better scenarios the agent goes straight to the goal, than the scenarios it keeps waling arround before it gets there.\n",
    "\n",
    "There is also an extra complexity on this evironment. The agent has 33% of chance that it will slip and move in a non expected direction.\n",
    "\n",
    "So let's see how to implement the changes we need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the observation space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space from frozen lake is just a number, from 0 to 15, that represents the index of the square the agent is on at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this kind of representation does not works well on machine learning, so we need to transform it to a format who works better for our models. We are going to use a technique called One Hot Encodding.\n",
    "\n",
    "I'll demo row to implement the One Hot Encodding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 9 Encodded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Original: 14 Encodded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Original: 13 Encodded: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Original: 2 Encodded: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Original: 4 Encodded: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "original_values = []\n",
    "encodded_values = []\n",
    "\n",
    "for _ in range(5):\n",
    "    obs = env.observation_space.sample() # we get a sample observation\n",
    "    one_hot = np.zeros(env.observation_space.n) # we create a one hot vector with all items as zero\n",
    "    one_hot[obs] = 1 # we set the value of the observation to 1\n",
    "\n",
    "    original_values.append(obs)\n",
    "    encodded_values.append(one_hot)\n",
    "\n",
    "for original, encodded in zip(original_values, encodded_values):\n",
    "    print(f'Original: {original} Encodded: {encodded}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, only the index related to our observation value should be set as one, and every other index keeps as zero. This way the model will understand better our observation data.\n",
    "\n",
    "So we need to force our environment to return the observation value in the format we need, and we can do that by creating a custom wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(OneHotWrapper, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " {'prob': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "env = OneHotWrapper(env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now our environment is returning the observation in the format we need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I'm not planning any change in the training_batch function, we can keep it as it is :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These named tuples are to store the experience of the agent\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "# this is the function to create the batches\n",
    "def training_batch(env: gym.Env, net: nn.Module, batch_size: int, device: str):\n",
    "    '''\n",
    "    This function will generate a batch of episodes from the environment.\n",
    "    The batch will be of size batch_size.\n",
    "    The neural network nn will be used to generate the actions.\n",
    "    The device will be used to store the tensors.\n",
    "\n",
    "    The function will yield a list of episodes of size batch_size.\n",
    "    '''\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    while True:\n",
    "        batch = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            obs,_ = env.reset()\n",
    "\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            total_reward = 0.0\n",
    "            steps = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "\n",
    "                while not terminated and not truncated:\n",
    "                    obs_v = torch.FloatTensor([obs]).to(device)\n",
    "\n",
    "                    act_probs_v = softmax(net(obs_v))\n",
    "                    act_probs = act_probs_v.data.cpu().numpy()[0]\n",
    "                    action = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "                    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                    steps.append(EpisodeStep(observation=obs, action=action))\n",
    "                    \n",
    "                    obs = next_obs\n",
    "                    total_reward += reward\n",
    "\n",
    "            batch.append(Episode(reward=total_reward, steps=steps))\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter elite episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 changed to do on this function:\n",
    "\n",
    "* Include a new parameter called gamma, to penalize rewards for episodes the agente took too much time to find the goal\n",
    "* Include a list of elite steps in the return\n",
    "\n",
    "The penalization in the reward is to reward better episodes the agent could meet the goal faster. So a agent who can meet the goal in 10 steps, will be rewarded better than an agent who spent 20 steps before meet the goal.\n",
    "\n",
    "Include the list of elite episodes in the return, give us the oportunity to reuse these steps and allow us to include them in the training process for long. This is needed because the episodes producing positive reward are going to be more rare on Frozen Lake than in it was on Cart Pole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These named tuples are to store the experience of the agent\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "def filter_batch_advanced(batch: list[Episode], device: str, gamma=0.9, percentile:int = 70):\n",
    "    '''\n",
    "    This function will filter the batch of episodes.\n",
    "    It will only keep the episodes that have a reward greater than the percentile.\n",
    "    It will return the observations and the actions of the episodes.\n",
    "    '''\n",
    "    rewards = [ e.reward * (gamma ** len(e.steps)) for e in batch ]\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean([ e.reward for e in batch ]))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_eps = [] # We need to keep the elite episodes a bit longer in the training process\n",
    "\n",
    "    for episode, reward in zip(batch, rewards):\n",
    "        if reward > reward_bound:\n",
    "            train_obs.extend([step.observation for step in episode.steps])\n",
    "            train_act.extend([step.action for step in episode.steps])\n",
    "            elite_eps.append(episode)\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs).to(device)\n",
    "    train_act_v = torch.LongTensor(train_act).to(device)\n",
    "\n",
    "    return train_obs_v, train_act_v, elite_eps, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep the same structure as the Cart Pole Neural Natwork. This NN should work fine as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we are already there! training the agent!\n",
    "\n",
    "Update the batch creation and filtering are the biggest changes we have to do! But since the frozen lake scenarion is a bit different than the cart pole, we will need to play a bit with the hiper parameters to ensure the agent will have the opportunity to meet the goal and find the present!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.377, reward_bound=0.0, success_rate=0.02, batch=5, max_success_rate=0.02, max_reward_bound=0.0\n",
      "50: loss=1.286, reward_bound=0.394, success_rate=0.028, batch=201, max_success_rate=0.048, max_reward_bound=0.463\n",
      "100: loss=1.146, reward_bound=0.341, success_rate=0.032, batch=199, max_success_rate=0.068, max_reward_bound=0.63\n",
      "150: loss=1.097, reward_bound=0.663, success_rate=0.036, batch=166, max_success_rate=0.08, max_reward_bound=0.663\n",
      "200: loss=1.067, reward_bound=0.54, success_rate=0.048, batch=202, max_success_rate=0.08, max_reward_bound=0.698\n",
      "250: loss=1.057, reward_bound=0.599, success_rate=0.064, batch=181, max_success_rate=0.108, max_reward_bound=0.698\n",
      "300: loss=1.042, reward_bound=0.57, success_rate=0.024, batch=203, max_success_rate=0.108, max_reward_bound=0.698\n",
      "350: loss=1.058, reward_bound=0.418, success_rate=0.06, batch=200, max_success_rate=0.108, max_reward_bound=0.698\n",
      "400: loss=0.99, reward_bound=0.54, success_rate=0.076, batch=200, max_success_rate=0.108, max_reward_bound=0.698\n",
      "450: loss=0.969, reward_bound=0.569, success_rate=0.1, batch=178, max_success_rate=0.116, max_reward_bound=0.698\n",
      "500: loss=0.972, reward_bound=0.44, success_rate=0.052, batch=198, max_success_rate=0.116, max_reward_bound=0.698\n",
      "550: loss=0.967, reward_bound=0.663, success_rate=0.04, batch=197, max_success_rate=0.116, max_reward_bound=0.698\n",
      "600: loss=0.937, reward_bound=0.0, success_rate=0.056, batch=178, max_success_rate=0.116, max_reward_bound=0.698\n",
      "650: loss=0.907, reward_bound=0.388, success_rate=0.08, batch=199, max_success_rate=0.124, max_reward_bound=0.698\n",
      "700: loss=0.884, reward_bound=0.488, success_rate=0.092, batch=195, max_success_rate=0.124, max_reward_bound=0.698\n",
      "750: loss=0.901, reward_bound=0.488, success_rate=0.084, batch=195, max_success_rate=0.124, max_reward_bound=0.698\n",
      "800: loss=0.877, reward_bound=0.0, success_rate=0.076, batch=67, max_success_rate=0.124, max_reward_bound=0.72\n",
      "850: loss=0.802, reward_bound=0.626, success_rate=0.1, batch=202, max_success_rate=0.124, max_reward_bound=0.72\n",
      "900: loss=0.736, reward_bound=0.46, success_rate=0.076, batch=193, max_success_rate=0.124, max_reward_bound=0.72\n",
      "950: loss=0.742, reward_bound=0.594, success_rate=0.088, batch=202, max_success_rate=0.136, max_reward_bound=0.72\n",
      "1000: loss=0.737, reward_bound=0.704, success_rate=0.068, batch=204, max_success_rate=0.136, max_reward_bound=0.72\n",
      "1050: loss=0.703, reward_bound=0.341, success_rate=0.104, batch=194, max_success_rate=0.148, max_reward_bound=0.72\n",
      "1100: loss=0.713, reward_bound=0.467, success_rate=0.092, batch=195, max_success_rate=0.152, max_reward_bound=0.72\n",
      "1150: loss=0.698, reward_bound=0.63, success_rate=0.104, batch=199, max_success_rate=0.16, max_reward_bound=0.72\n",
      "1200: loss=0.689, reward_bound=0.569, success_rate=0.12, batch=196, max_success_rate=0.16, max_reward_bound=0.72\n",
      "1250: loss=0.657, reward_bound=0.599, success_rate=0.132, batch=163, max_success_rate=0.16, max_reward_bound=0.72\n",
      "1300: loss=0.618, reward_bound=0.662, success_rate=0.112, batch=203, max_success_rate=0.18, max_reward_bound=0.72\n",
      "1350: loss=0.602, reward_bound=0.63, success_rate=0.14, batch=197, max_success_rate=0.18, max_reward_bound=0.72\n",
      "1400: loss=0.612, reward_bound=0.599, success_rate=0.136, batch=200, max_success_rate=0.18, max_reward_bound=0.72\n",
      "1450: loss=0.606, reward_bound=0.599, success_rate=0.132, batch=197, max_success_rate=0.18, max_reward_bound=0.72\n",
      "1500: loss=0.59, reward_bound=0.63, success_rate=0.108, batch=201, max_success_rate=0.18, max_reward_bound=0.72\n",
      "1550: loss=0.563, reward_bound=0.442, success_rate=0.14, batch=199, max_success_rate=0.192, max_reward_bound=0.72\n",
      "1600: loss=0.573, reward_bound=0.599, success_rate=0.14, batch=195, max_success_rate=0.192, max_reward_bound=0.72\n",
      "1650: loss=0.52, reward_bound=0.463, success_rate=0.144, batch=193, max_success_rate=0.192, max_reward_bound=0.72\n",
      "1700: loss=0.53, reward_bound=0.63, success_rate=0.172, batch=191, max_success_rate=0.196, max_reward_bound=0.72\n",
      "1750: loss=0.56, reward_bound=0.63, success_rate=0.188, batch=185, max_success_rate=0.228, max_reward_bound=0.72\n",
      "1800: loss=0.552, reward_bound=0.569, success_rate=0.148, batch=194, max_success_rate=0.228, max_reward_bound=0.72\n",
      "1850: loss=0.468, reward_bound=0.0, success_rate=0.164, batch=162, max_success_rate=0.228, max_reward_bound=0.72\n",
      "1900: loss=0.473, reward_bound=0.463, success_rate=0.2, batch=194, max_success_rate=0.248, max_reward_bound=0.72\n",
      "1950: loss=0.473, reward_bound=0.474, success_rate=0.216, batch=189, max_success_rate=0.248, max_reward_bound=0.72\n",
      "2000: loss=0.457, reward_bound=0.569, success_rate=0.168, batch=198, max_success_rate=0.252, max_reward_bound=0.72\n",
      "2050: loss=0.487, reward_bound=0.599, success_rate=0.228, batch=145, max_success_rate=0.26, max_reward_bound=0.72\n",
      "2100: loss=0.471, reward_bound=0.569, success_rate=0.208, batch=197, max_success_rate=0.26, max_reward_bound=0.72\n",
      "2150: loss=0.429, reward_bound=0.513, success_rate=0.188, batch=188, max_success_rate=0.264, max_reward_bound=0.72\n",
      "2200: loss=0.434, reward_bound=0.663, success_rate=0.144, batch=201, max_success_rate=0.264, max_reward_bound=0.72\n",
      "2250: loss=0.457, reward_bound=0.569, success_rate=0.204, batch=199, max_success_rate=0.264, max_reward_bound=0.72\n",
      "2300: loss=0.477, reward_bound=0.663, success_rate=0.168, batch=188, max_success_rate=0.28, max_reward_bound=0.72\n",
      "2350: loss=0.479, reward_bound=0.63, success_rate=0.272, batch=199, max_success_rate=0.28, max_reward_bound=0.72\n",
      "2400: loss=0.474, reward_bound=0.605, success_rate=0.208, batch=200, max_success_rate=0.28, max_reward_bound=0.72\n",
      "2450: loss=0.425, reward_bound=0.312, success_rate=0.208, batch=177, max_success_rate=0.28, max_reward_bound=0.72\n",
      "2500: loss=0.435, reward_bound=0.652, success_rate=0.3, batch=200, max_success_rate=0.32, max_reward_bound=0.72\n",
      "2550: loss=0.366, reward_bound=0.565, success_rate=0.228, batch=202, max_success_rate=0.32, max_reward_bound=0.72\n",
      "2600: loss=0.382, reward_bound=0.569, success_rate=0.22, batch=196, max_success_rate=0.32, max_reward_bound=0.72\n",
      "2650: loss=0.372, reward_bound=0.54, success_rate=0.336, batch=190, max_success_rate=0.348, max_reward_bound=0.72\n",
      "2700: loss=0.342, reward_bound=0.44, success_rate=0.312, batch=188, max_success_rate=0.372, max_reward_bound=0.72\n",
      "2750: loss=0.334, reward_bound=0.605, success_rate=0.224, batch=200, max_success_rate=0.376, max_reward_bound=0.72\n",
      "2800: loss=0.29, reward_bound=0.142, success_rate=0.292, batch=176, max_success_rate=0.376, max_reward_bound=0.72\n",
      "2850: loss=0.3, reward_bound=0.599, success_rate=0.344, batch=179, max_success_rate=0.404, max_reward_bound=0.72\n",
      "2900: loss=0.282, reward_bound=0.599, success_rate=0.364, batch=195, max_success_rate=0.404, max_reward_bound=0.72\n",
      "2950: loss=0.268, reward_bound=0.663, success_rate=0.316, batch=187, max_success_rate=0.416, max_reward_bound=0.72\n",
      "3000: loss=0.27, reward_bound=0.684, success_rate=0.292, batch=204, max_success_rate=0.42, max_reward_bound=0.72\n",
      "3050: loss=0.246, reward_bound=0.54, success_rate=0.348, batch=185, max_success_rate=0.432, max_reward_bound=0.72\n",
      "3100: loss=0.232, reward_bound=0.488, success_rate=0.424, batch=188, max_success_rate=0.48, max_reward_bound=0.72\n",
      "3150: loss=0.247, reward_bound=0.599, success_rate=0.412, batch=194, max_success_rate=0.48, max_reward_bound=0.72\n",
      "3200: loss=0.227, reward_bound=0.63, success_rate=0.364, batch=202, max_success_rate=0.48, max_reward_bound=0.72\n",
      "3250: loss=0.202, reward_bound=0.513, success_rate=0.404, batch=186, max_success_rate=0.48, max_reward_bound=0.72\n",
      "3300: loss=0.185, reward_bound=0.599, success_rate=0.36, batch=196, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3350: loss=0.208, reward_bound=0.698, success_rate=0.448, batch=139, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3400: loss=0.208, reward_bound=0.626, success_rate=0.336, batch=202, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3450: loss=0.206, reward_bound=0.587, success_rate=0.42, batch=195, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3500: loss=0.194, reward_bound=0.488, success_rate=0.408, batch=196, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3550: loss=0.21, reward_bound=0.307, success_rate=0.372, batch=169, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3600: loss=0.225, reward_bound=0.599, success_rate=0.316, batch=196, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3650: loss=0.195, reward_bound=0.44, success_rate=0.392, batch=184, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3700: loss=0.171, reward_bound=0.63, success_rate=0.364, batch=197, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3750: loss=0.21, reward_bound=0.44, success_rate=0.324, batch=191, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3800: loss=0.199, reward_bound=0.54, success_rate=0.44, batch=188, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3850: loss=0.175, reward_bound=0.0, success_rate=0.344, batch=86, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3900: loss=0.177, reward_bound=0.63, success_rate=0.472, batch=180, max_success_rate=0.5, max_reward_bound=0.72\n",
      "3950: loss=0.193, reward_bound=0.63, success_rate=0.384, batch=186, max_success_rate=0.5, max_reward_bound=0.72\n",
      "4000: loss=0.211, reward_bound=0.599, success_rate=0.412, batch=187, max_success_rate=0.5, max_reward_bound=0.72\n",
      "4050: loss=0.204, reward_bound=0.242, success_rate=0.344, batch=183, max_success_rate=0.5, max_reward_bound=0.72\n",
      "4100: loss=0.205, reward_bound=0.513, success_rate=0.396, batch=194, max_success_rate=0.5, max_reward_bound=0.72\n",
      "4150: loss=0.175, reward_bound=0.607, success_rate=0.448, batch=196, max_success_rate=0.5, max_reward_bound=0.72\n",
      "4200: loss=0.194, reward_bound=0.569, success_rate=0.404, batch=192, max_success_rate=0.5, max_reward_bound=0.72\n",
      "4250: loss=0.193, reward_bound=0.663, success_rate=0.364, batch=199, max_success_rate=0.5, max_reward_bound=0.72\n",
      "4300: loss=0.208, reward_bound=0.569, success_rate=0.404, batch=200, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4350: loss=0.203, reward_bound=0.463, success_rate=0.408, batch=185, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4400: loss=0.195, reward_bound=0.663, success_rate=0.412, batch=201, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4450: loss=0.202, reward_bound=0.175, success_rate=0.356, batch=176, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4500: loss=0.209, reward_bound=0.599, success_rate=0.356, batch=179, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4550: loss=0.215, reward_bound=0.663, success_rate=0.348, batch=141, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4600: loss=0.234, reward_bound=0.569, success_rate=0.276, batch=193, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4650: loss=0.217, reward_bound=0.599, success_rate=0.308, batch=195, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4700: loss=0.217, reward_bound=0.63, success_rate=0.336, batch=192, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4750: loss=0.212, reward_bound=0.54, success_rate=0.36, batch=192, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4800: loss=0.205, reward_bound=0.569, success_rate=0.332, batch=196, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4850: loss=0.208, reward_bound=0.697, success_rate=0.336, batch=203, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4900: loss=0.17, reward_bound=0.44, success_rate=0.288, batch=197, max_success_rate=0.504, max_reward_bound=0.72\n",
      "4950: loss=0.169, reward_bound=0.63, success_rate=0.408, batch=157, max_success_rate=0.504, max_reward_bound=0.72\n",
      "5000: loss=0.181, reward_bound=0.593, success_rate=0.336, batch=197, max_success_rate=0.504, max_reward_bound=0.72\n",
      "5050: loss=0.172, reward_bound=0.0, success_rate=0.312, batch=78, max_success_rate=0.504, max_reward_bound=0.72\n",
      "5100: loss=0.166, reward_bound=0.599, success_rate=0.404, batch=182, max_success_rate=0.504, max_reward_bound=0.72\n",
      "5117: loss=0.153, reward_bound=0.607, success_rate=0.4, batch=196, max_success_rate=0.504, max_reward_bound=0.72\n",
      "Time limit reached\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# DEFINE THE HYPERPARAMETERS\n",
    "NN_HIDDEN_SIZE = 192\n",
    "LEARNING_RATE = 0.001 # we need to lower the learning rate\n",
    "BATCH_SIZE = 250 # we need more samples now, as just a fee will have a reward > 0\n",
    "GAMMA = 0.95 # we add the gamma hyperparameter to penalize the reward\n",
    "PERCENTILE = 55 # we lower the percentile to increace the chance of have positive rewards\n",
    "\n",
    "# DEFINE THE ENVIRONMENT\n",
    "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "env = OneHotWrapper(env) # we add the one hot wrapper\n",
    "\n",
    "episode_trigger = lambda x: x % 200000 == 0\n",
    "env = RecordGif(env, './gifs/cross-entroph/frozen-lake', name_prefix='training', gif_length=200, fps=5, episode_trigger=episode_trigger)\n",
    "\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# DEFINE THE NETWORK\n",
    "net = Net(observation_size, NN_HIDDEN_SIZE, action_size)\n",
    "\n",
    "# DEFINE THE OPTIMIZER\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# DEFINE THE LOSS FUNCTION\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# DEFINE THE DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net.to(device)\n",
    "\n",
    "# DEFINE THE WRITER\n",
    "writer = SummaryWriter(logdir='runs/cross-entroph/frozen-lake', comment=f'-frozen-lake-pg')\n",
    "\n",
    "# TRAIN THE AGENT\n",
    "starting_time = datetime.datetime.now()\n",
    "\n",
    "max_success_rate = 0.0\n",
    "max_reward_bound = 0.0\n",
    "full_batch = []\n",
    "for iter_n, batch in enumerate(training_batch(env, net, BATCH_SIZE, device)):\n",
    "    success_rate = sum([ e.reward for e in batch ]) / BATCH_SIZE\n",
    "    max_success_rate = max(max_success_rate, success_rate)\n",
    "\n",
    "    if  success_rate > 0.55:\n",
    "        print(f'{iter_n}: loss={round(loss_v.item(), 3)}, reward_bound={round(reward_b, 3)},'\n",
    "              f' success_rate={round(success_rate, 3)}, batch={len(full_batch)},'\n",
    "              f' max_success_rate={round(max_success_rate, 3)}, max_reward_bound={round(max_reward_bound, 3)}')\n",
    "        print('Solved!')\n",
    "        break\n",
    "\n",
    "\n",
    "    obs_v, acts_v, full_batch, reward_b, _ = filter_batch_advanced(full_batch+batch, device, gamma=GAMMA, percentile=PERCENTILE)\n",
    "\n",
    "    if not len(full_batch):\n",
    "        continue\n",
    "\n",
    "    max_reward_bound = max(max_reward_bound, reward_b)\n",
    "    full_batch = full_batch[-500:] # we keep the most recent elite episodes\n",
    "\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = loss_fn(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    writer.add_scalar('loss', loss_v.item(), iter_n)\n",
    "    writer.add_scalar('reward_bound', reward_b, iter_n)\n",
    "    writer.add_scalar('success_rate', success_rate, iter_n)\n",
    "\n",
    "    if iter_n % 50 == 0:\n",
    "        print(f'{iter_n}: loss={round(loss_v.item(), 3)}, reward_bound={round(reward_b, 3)},'\n",
    "              f' success_rate={round(success_rate, 3)}, batch={len(full_batch)},'\n",
    "              f' max_success_rate={round(max_success_rate, 3)}, max_reward_bound={round(max_reward_bound, 3)}')\n",
    "\n",
    "    # we add a time limit to the training\n",
    "    # it should not take more than 4 hours\n",
    "    delta_time = datetime.datetime.now() - starting_time\n",
    "    if delta_time.total_seconds() > 3600 * 4:\n",
    "        print(f'{iter_n}: loss={round(loss_v.item(), 3)}, reward_bound={round(reward_b, 3)},'\n",
    "              f' success_rate={round(success_rate, 3)}, batch={len(full_batch)},'\n",
    "              f' max_success_rate={round(max_success_rate, 3)}, max_reward_bound={round(max_reward_bound, 3)}')\n",
    "        print('Time limit reached')\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking the metrics on tensorboad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorboard is a tool used to record and read metrics from our models. Let's see our metrics!\n",
    "\n",
    "<img src=\"prints/cross-entroph/tensorboard-frozen-lake.png\" style=\"width: 1000px\">\n",
    "\n",
    "As expected, we can see the loss decreasing over the time, but the reward is not consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "Path('models/cross-entroph').mkdir(exist_ok=True, parents=True)\n",
    "torch.save(net.state_dict(), 'models/cross-entroph/frozen-lake-pg.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is a good example of scenarion the Cross-Entrophy learning does not performs pretty well. Even after hours os training, it will not have a success rate much better than 50% os the test executions.\n",
    "\n",
    "For this kind of scenarios we will have better techiniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a final video of the agent playing the frozen lake game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "env = OneHotWrapper(env) # we add the one hot wrapper\n",
    "env = RecordGif(env, './gifs/cross-entroph/frozen-lake', name_prefix='model', gif_length=500)\n",
    "\n",
    "net.load_state_dict(torch.load('models/cross-entroph/frozen-lake-pg.pth'))\n",
    "net.eval()\n",
    "\n",
    "obs,_ = env.reset(seed=42)\n",
    "while True:\n",
    "    obs_v = torch.FloatTensor([obs]).to(device)\n",
    "    action = torch.argmax(net(obs_v)).item()\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"gifs/cross-entroph/frozen-lake/model-episode-0.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... at least it could get the prize :|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Cross-Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy is a way to measure how different two probability distributions are. Imagine you have a model that makes predictions, and you want to see how close these predictions are to the actual outcomes. Cross-Entropy gives a number that tells you how good your model is. A lower cross-entropy means your model's predictions are better.\n",
    "\n",
    "**Simple Example:**\n",
    "\n",
    "* Think of predicting the weather. If you say there's a 70% chance of rain and it actually rains, your prediction is pretty good. Cross-Entropy helps measure how accurate such predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does RL Use Cross-Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Reinforcement Learning (RL), an agent learns to make decisions by performing actions and receiving rewards. When RL uses Cross-Entropy, it helps the agent improve its actions based on past experiences.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "* The agent tries different actions in an environment.\n",
    "* It records which actions lead to good rewards.\n",
    "* Using Cross-Entropy, it focuses more on the actions that gave better rewards.\n",
    "* Over time, the agent gets better at choosing actions that maximize rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Step-by-Step Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a simple step-by-step guide on how RL with Cross-Entropy works:\n",
    "\n",
    "1. **Initialize:** Start with a random policy (a set of rules the agent follows to decide actions).\n",
    "2. **Generate Episodes:** Let the agent perform actions in the environment to create episodes (sequences of states, actions, and rewards).\n",
    "3. **Evaluate:** Calculate the total rewards for each episode.\n",
    "Select Top Performers: Choose the best-performing episodes based on their rewards.\n",
    "4. **Update Policy:** Use Cross-Entropy to update the policy, making it more likely to choose actions that led to high rewards.\n",
    "5. **Repeat:** Go back to step 2 and repeat the process until the agent performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantages:\n",
    "    * Simplicity: The Cross-Entropy method is straightforward and easy to implement.\n",
    "    * Efficiency: It can find good solutions with fewer trials compared to other methods.\n",
    "\n",
    "\n",
    "* Applications:\n",
    "    * Robotics (teaching robots to perform tasks)\n",
    "    * Game Playing (like training agents to play video games)\n",
    "    * Optimization Problems (finding the best solutions in complex scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I would like to check the graphics card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  9 23:17:43 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   67C    P8              3W /   60W |     188MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      9072    C+G   ...pplications\\Tryd6\\jre\\bin\\javaw.exe      N/A      |\n",
      "|    0   N/A  N/A      9992    C+G   ...ROGRA~2\\Citrix\\ICACLI~1\\wfica32.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Sep_12_02:55:00_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.77\n",
      "Build cuda_12.6.r12.6/compiler.34841621_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n",
      "Is CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print('Is CUDA available: ' + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from tensorboardX import SummaryWriter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CartPole is one of the most basic environments available on gymnasium. This environment is well suit suitable to be used on this first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from utils.wrappers import RecordGif\n",
    "\n",
    "# RUN A RANDOM AGENT FOR DEMO PURPOSES\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env = RecordGif(env, f'./gifs/cartpole', gif_length=200, name_prefix='random-agent')\n",
    "\n",
    "env.reset(seed=42)\n",
    "for _ in range(200):\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"gifs/cartpole/random-agent-episode-0.gif\" style=\"width: 350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a video of the cart pole environment being controlled by random actions.<br>\n",
    "This environment consists in a pole being balanced by a cart who can move to left or right to keep the pole in a vertical position.<br>\n",
    "The apisode ends when the pole cross a given angle, or the cart cross the environment boundaries. For every step the agent can keep the pole balanced, it will receive a positive reward.\n",
    "\n",
    "\n",
    "The environment could look simple, but it is a good candidate to implement a reinforcement learning, cross-entropy, model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the function to yield the batches to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to create the batches of data to train the model. The batches are set by running the Neural Network throught the environment steps, acumulate the rewards and group them in collections to use during the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These named tuples are to store the experience of the agent\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "# this is the function to create the batches\n",
    "def training_batch(env: gym.Env, net: nn.Module, batch_size: int, device: str):\n",
    "    '''\n",
    "    This function will generate a batch of episodes from the environment.\n",
    "    The batch will be of size batch_size.\n",
    "    The neural network nn will be used to generate the actions.\n",
    "    The device will be used to store the tensors.\n",
    "\n",
    "    The function will yield a list of episodes of size batch_size.\n",
    "    '''\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    batchs = []\n",
    "\n",
    "    while True:\n",
    "        for _ in range(batch_size):\n",
    "            obs,_ = env.reset()\n",
    "\n",
    "            total_reward = 0.0\n",
    "            steps = []\n",
    "\n",
    "            while True:\n",
    "                obs_v = torch.FloatTensor([obs]).to(device)\n",
    "\n",
    "                act_probs_v = softmax(net(obs_v))\n",
    "                act_probs = act_probs_v.data.cpu().numpy()[0]\n",
    "                action = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "                next_obs, reward, done, truncated, _ = env.step(action)\n",
    "                steps.append(EpisodeStep(observation=obs, action=action))\n",
    "                \n",
    "                obs = next_obs\n",
    "                total_reward += reward\n",
    "\n",
    "                if done or truncated:\n",
    "                    batchs.append(Episode(reward=total_reward, steps=steps))\n",
    "                    break\n",
    "        \n",
    "        yield batchs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Episode**: This tuple was defined to store the information of every episode. So for every run where our neural network is interacting with the environment we are acumulating the data in a instance of this tuple. The 'reward' field receives the total reward obtained during the episode, and the 'steps' field is to store a list of *EpisodeStep*.\n",
    "\n",
    "**EpisodeStep**: This tuple was defined to store the information of a single step from the agent in the environment. So for every step we needs to create a new instance of this tuple to include in the *Episode*. The 'action' field is to store the action from the agent in the step, and the 'observation' field is to store the observation the agend did use to choose the action.\n",
    "\n",
    "> It is important to notice here, the opservation we are includding in the EpisodeStep is the 'obs' the agent did use to choose an action and not the 'next_obs' we receive after the step.\n",
    "\n",
    "**training_batch**: This function has the singlçe objective of run some episodes using the Neural Network agent to choose an action for every step from the episodes. Than it should acumulate the data from the episodes and return it as an yield value. We can use this function to get the batches we need during the training of the agent.\n",
    "\n",
    "> You can see something interesting in the training_batch function. When we are choosing an action to step into the evrironment, we are not using the direct result from the Neural Network, we are using the probabilities generated by the Softmax activation function, and we are using these probabilities to choose a random action based on them. So while the agente is getting more confident to perform an action for a given scenario, this action will have more chances to be the chosen while we keep space for exploration.\n",
    "\n",
    "> Another interesting point is we are using the Softmax outside of the newral network! We are doing it, because we not expecting to use it on our final agent, but it is supporting us to get the probabilities for every action, for every step, during the batch creation.\n",
    "\n",
    "\n",
    "Ok, now we have a way to get batches to use during the training. But we still have to filter the top episodes to discard the ones with a bad performance. This is exactly what we are going to do next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the episodes to use for model training (choose the Elite episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our agent will learn on good experences and ignore the bad ones, we have to... we... discard the bad experiences! We are looking for the Elite ones!\n",
    "\n",
    "A technique to filter off the bad experiences is to define a minimun reward for an episode to include it on our training data. But should not be just an abritary value, and it has to adapt for every batch, since the average performance should increace over the time.\n",
    "\n",
    "We can, instead of an abitrary value, organize the rewards in percentiles, and choose only the episodes whos reward is equals or higger than the value related to a given percentile we choose. So with this strategy, the value we use to filter episodes will adapt for every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch: list[Episode], device: str, percentile:int = 70):\n",
    "    '''\n",
    "    This function will filter the batch of episodes.\n",
    "    It will only keep the episodes that have a reward greater than the percentile.\n",
    "    It will return the observations and the actions of the episodes.\n",
    "    '''\n",
    "    rewards = [ e.reward for e in batch ]\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "\n",
    "    for episode in batch:\n",
    "        if episode.reward <= reward_bound:\n",
    "            continue\n",
    "\n",
    "        train_obs.extend([step.observation for step in episode.steps])\n",
    "        train_act.extend([step.action for step in episode.steps])\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs).to(device)\n",
    "    train_act_v = torch.LongTensor(train_act).to(device)\n",
    "\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our function to filter batches is going to find the reward value related to the percentile (70 defined as default), use this percentile to filter episodes, acumulate the opservations and actions and return the observation, action, the reward bound (the reward related to the given percentile) and the reward mean.\n",
    "\n",
    "The reard bound and mean are just for metrics and they don't have any direct impact in the agent itself (well... we are going to use reward mean for early stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent (A Neural Network responsible by learn and run the episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define our agent! This is basically a neural retwork who is going to learn with experiences and play the episodes of Cart Pole game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Newral Network is pretty simple (we don't need anything to complex for Cart Pole game), we have 2 linear models, connected by a ReLU activation function.\n",
    "\n",
    "The model definition is arbritary and could be structured in different ways, but this one should work for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now all the pieces we need to start training our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.7048795223236084, reward_bound=18.5, reward_mean=17.125\n",
      "25: loss=0.6216868162155151, reward_bound=70.0, reward_mean=58.37019230769231\n",
      "50: loss=0.6045562624931335, reward_bound=119.5, reward_mean=92.32230392156863\n",
      "75: loss=0.5970025658607483, reward_bound=154.0, reward_mean=121.57565789473684\n",
      "100: loss=0.5938321352005005, reward_bound=180.0, reward_mean=144.88737623762376\n",
      "125: loss=0.5911875367164612, reward_bound=206.0, reward_mean=169.7514880952381\n",
      "150: loss=0.5886478424072266, reward_bound=236.0, reward_mean=193.9788907284768\n",
      "156: loss=0.5883484482765198, reward_bound=244.0, reward_mean=199.1421178343949\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "# DEFINE THE HYPERPARAMETERS\n",
    "NN_HIDDEN_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# DEFINE THE ENVIRONMENT\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env = RecordGif(env, './gifs/cartpole', name_prefix='training', gif_length=200, episode_trigger=lambda x: x % 500 == 0)\n",
    "\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# DEFINE THE NETWORK\n",
    "net = Net(observation_size, NN_HIDDEN_SIZE, action_size)\n",
    "\n",
    "# DEFINE THE OPTIMIZER\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# DEFINE THE LOSS FUNCTION\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# DEFINE THE DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net.to(device)\n",
    "\n",
    "# DEFINE THE WRITER\n",
    "writer = SummaryWriter(logdir='runs/cart_pole', comment=f'-cartpole-pg')\n",
    "\n",
    "# TRAIN THE AGENT\n",
    "for iter_n, batch in enumerate(training_batch(env, net, BATCH_SIZE, device)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = loss_fn(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter_n % 25 == 0:\n",
    "        print(f'{iter_n}: loss={loss_v.item()}, reward_bound={reward_b}, reward_mean={reward_m}')\n",
    "\n",
    "    writer.add_scalar('loss', loss_v.item(), iter_n)\n",
    "    writer.add_scalar('reward_bound', reward_b, iter_n)\n",
    "    writer.add_scalar('reward_mean', reward_m, iter_n)\n",
    "\n",
    "    if reward_m > 199:\n",
    "        print(f'{iter_n}: loss={loss_v.item()}, reward_bound={reward_b}, reward_mean={reward_m}')\n",
    "        print('Solved!')\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took a while, but we have an Agent who can play the Cart Pole!\n",
    "\n",
    "How about compare some episodes?!\n",
    "\n",
    "**Episode 0:**<br>\n",
    "<image src=\"gifs/cartpole/training-episode-0.gif\" style=\"width: 250px\">\n",
    "\n",
    "**Episode 1000:**<br>\n",
    "<image src=\"gifs/cartpole/training-episode-1000.gif\" style=\"width: 250px\">\n",
    "\n",
    "**Episode 2500:**<br>\n",
    "<image src=\"gifs/cartpole/training-episode-2500.gif\" style=\"width: 250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking the metrics on tensorboad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorboard is a tool used to record and read metrics from our models. Let's see our metrics!\n",
    "\n",
    "<img src=\"./prints/tensorboard-cartpole.png\" style=\"width: 1000px\">\n",
    "\n",
    "Using the tensorboard we can see the loss going down and the reward going up (reward is increacing almost linear hahahaha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "torch.save(net.state_dict(), 'models/cartpole-pg.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a final video of the agent playing the cart pole game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "env = RecordGif(env, './gifs/cartpole', name_prefix='model', gif_length=500)\n",
    "\n",
    "net.load_state_dict(torch.load('models/cartpole-pg.pth'))\n",
    "net.eval()\n",
    "\n",
    "obs,_ = env.reset(seed=42)\n",
    "while True:\n",
    "    obs_v = torch.FloatTensor([obs]).to(device)\n",
    "    action = torch.argmax(net(obs_v)).item()\n",
    "    obs, reward, done, truncated, _ = env.step(action)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"gifs/cartpole/model-episode-0.gif\" style=\"width: 350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is quite good!<br>\n",
    "It may not be able to run indefinidelly for some scenarios, since the agent never had to care about what happens after the 200 steps we did set as the reward mean to stop the training. Even with the limitation, we could produce a good agent!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
